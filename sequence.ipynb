{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2xWYAZFX2L/pg/6QPuueT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","path_final = \"/content/drive/MyDrive/LTSSUD\"\n","import os\n","\n","if (os.path.isdir(path_final) == True):\n","  %cd \"/content/drive/MyDrive/LTSSUD\"\n","\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAqJ1V0bWJpt","executionInfo":{"status":"ok","timestamp":1688435516904,"user_tz":-420,"elapsed":2453,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}},"outputId":"6d8d2ae6-8592-4876-ff09-2ab184bdd3fd"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/LTSSUD\n","Proposal.gdoc\ttest.csv   Untitled0.ipynb\n","sequence.ipynb\ttrain.csv  winequality-red.csv\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import math\n","\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute  import SimpleImputer\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"jjLREnOAUH6r","executionInfo":{"status":"ok","timestamp":1688435516904,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["class XGBoostTree:\n","    def __init__(self, max_depth=3, min_samples_split=2, min_impurity=1e-7, gamma = 0):\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.min_impurity = min_impurity\n","        self.gamma = gamma\n","        self.tree = {}\n","\n","    def calculate_gradient(self, y_true, y_pred):\n","        p = np.exp(y_pred) / (1 + np.exp(y_pred))\n","        return p - y_true\n","\n","    def calculate_hessian(self, y_true, y_pred):\n","        p = np.exp(y_pred) / (1 + np.exp(y_pred))\n","        return p * (1 - p)\n","\n","    def split_data(self, X, feature_index, split_value):\n","        left_indices = X[:, feature_index] <= split_value\n","        right_indices = X[:, feature_index] > split_value\n","        return left_indices, right_indices\n","\n","    def similarity(self, y_true, p, i = 2):\n","        numerator = np.sum(y_true) ** i\n","        denominator = np.sum(p * (1 - p)) + self.min_impurity\n","        return numerator / denominator\n","\n","    def find_best_split(self, X, y, props):\n","        best_gain = -np.inf\n","        best_split_feature = None\n","        best_split_value = None\n","\n","        num_samples, num_features = X.shape\n","\n","        for feature_index in range(num_features):\n","            feature_values = X[:, feature_index]\n","            unique_values = np.unique(feature_values)\n","\n","            for value in unique_values:\n","                left_indices, right_indices = self.split_data(X, feature_index, value)\n","                if len(left_indices) < self.min_samples_split or len(right_indices) < self.min_samples_split:\n","                    continue\n","\n","                y_left = y[left_indices]\n","                y_right = y[right_indices]\n","                p_left = props[left_indices]\n","                p_right = props[right_indices]\n","\n","                gain = self.similarity(y_left, p_left) + self.similarity(y_right, p_right) - self.similarity(y, props)\n","\n","                if gain > best_gain:\n","                    best_gain = gain\n","                    best_split_feature = feature_index\n","                    best_split_value = value\n","        if(best_gain - self.gamma < 0):\n","            best_split_feature = None\n","            best_split_value = None\n","\n","        return best_split_feature, best_split_value\n","\n","    def create_leaf_node(self, y, props):\n","        return self.similarity(y, props, 1)\n","\n","    def build_tree(self, X, y, props, depth=0):\n","        if depth >= self.max_depth or len(X) < self.min_samples_split:\n","            return self.create_leaf_node(y, props)\n","\n","        split_feature, split_value = self.find_best_split(X, y, props)\n","\n","        if split_feature is None:\n","            return self.create_leaf_node(y, props)\n","\n","        left_indices, right_indices = self.split_data(X, split_feature, split_value)\n","        left_child = self.build_tree(X[left_indices], y[left_indices], props[left_indices], depth + 1)\n","        right_child = self.build_tree(X[right_indices], y[right_indices], props[right_indices], depth + 1)\n","\n","        self.tree = {\n","            'split_feature': split_feature,\n","            'split_value': split_value,\n","            'left_child': left_child,\n","            'right_child': right_child\n","        }\n","\n","        return self.tree\n","\n","    def fit(self, X, y, props):\n","        self.tree = self.build_tree(X, y, props)\n","\n","    def predict(self, X):\n","        return np.array([self._traverse_tree(x, self.tree) for x in X])\n","\n","    def _traverse_tree(self, x, node):\n","        if isinstance(node, dict):\n","            split_feature = node['split_feature']\n","            split_value = node['split_value']\n","            if x[split_feature] <= split_value:\n","                return self._traverse_tree(x, node['left_child'])\n","            else:\n","                return self._traverse_tree(x, node['right_child'])\n","        else:\n","            return node"],"metadata":{"id":"NWGpSJ29bVr4","executionInfo":{"status":"ok","timestamp":1688435516904,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Define the loss function (e.g., mean squared error)\n","def mean_squared_error(y_true, y_pred):\n","    return np.mean((y_true - y_pred) ** 2)\n","\n","\n","def residuals(y_true, y_pred):\n","    return (y_true - y_pred)\n","\n","\n","def build_weak_learner(X, y, props, min_impurity, gamma):\n","    # Create the weak learner (decision tree) and fit it to the gradients and Hessians\n","    model = XGBoostTree(min_impurity = min_impurity, gamma = gamma)  # Adjust the hyperparameters as needed\n","    model.fit(X, y, props)  # Divide gradients by Hessians to account for second-order effects\n","    return model\n","\n","# Define the XGBoost model\n","class XGBoostModel:\n","    def __init__(self, n_estimators, learning_rate, min_impurity = 1e-7, gamma = 0):\n","        self.n_estimators = n_estimators\n","        self.learning_rate = learning_rate\n","        self.initial_prediction = 0\n","        self.min = None\n","        self.max = None\n","        self.min_impurity = min_impurity\n","        self.gamma = gamma\n","        self.models = []\n","\n","    def fit(self, X, y):\n","        # Convert data to DMatrix format\n","        #data = pd.concat([X, y], axis=1)\n","        data = X\n","        #dmatrix = DMatrix(data.values)\n","        self.min = np.min(y)\n","        self.max = np.max(y)\n","        y = (y - self.min)/(self.max - self.min)\n","\n","        classes, counts = np.unique(y, return_counts=True)\n","        #dominant_class = classes[np.argmax(counts)]\n","        predictions = np.full(len(y), counts[np.argmax(counts)]/np.sum(counts))\n","        self.initial_prediction = counts[np.argmax(counts)]/np.sum(counts)\n","        #print(self.initial_prediction)\n","        # Build the models in a loop\n","        for _ in tqdm(range(self.n_estimators)):\n","            props = predictions\n","            residual = residuals(y, predictions)\n","\n","            # Fit a weak learner (e.g., a decision tree) to the gradients and Hessians\n","            model = build_weak_learner(data, residual, props, self.min_impurity, self.gamma)  # Implement your own weak learner\n","\n","            # Update the predictions using the learning rate and the predictions of the weak learner\n","            t = np.log(predictions/(1-predictions)) + self.learning_rate * model.predict(data)\n","            predictions = np.exp(t) / (1 + np.exp(t))\n","\n","            # Add the model to the ensemble\n","            self.models.append(model)\n","\n","    def predict(self, X):\n","        # Make predictions by aggregating the predictions of all models in the ensemble\n","        predictions = np.full(len(X),self.initial_prediction)\n","        #print(predictions)\n","        for model in self.models:\n","            t = np.log(predictions/(1-predictions)) + self.learning_rate * model.predict(X)\n","            predictions = np.exp(t) / (1 + np.exp(t))\n","        predictions = predictions*(self.max - self.min) + self.min\n","        return np.around(predictions)"],"metadata":{"id":"b30KzBDXJuy2","executionInfo":{"status":"ok","timestamp":1688435517366,"user_tz":-420,"elapsed":471,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","dat = pd.read_csv('winequality-red.csv')\n","dat = dat.dropna(subset=['quality'])\n","y = dat['quality']= le.fit_transform(dat['quality'])\n","X = dat.drop(['quality'], axis=1).select_dtypes(exclude=['object'])\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25)\n","train_X.fillna(0, inplace = True)\n","test_X.fillna(0, inplace = True)\n","my_imputer = SimpleImputer()\n","train_X = my_imputer.fit_transform(train_X)\n","test_X = my_imputer.transform(test_X)\n","#train_X = np.rot90(train_X)\n","#test_X = np.rot90(test_X)"],"metadata":{"id":"WMQ29TG2RXyA","executionInfo":{"status":"ok","timestamp":1688435517367,"user_tz":-420,"elapsed":472,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Create and train the XGBoost model\n","xgb_model = XGBoostModel(n_estimators=100, learning_rate=0.1, min_impurity = 0, gamma = 0)\n","xgb_model.fit(train_X, train_y)"],"metadata":{"id":"lgJCLd4RULKs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe4c8bf5-fd04-411e-d368-9dea95255be9","executionInfo":{"status":"ok","timestamp":1688435546620,"user_tz":-420,"elapsed":29257,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":48,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:29<00:00,  3.38it/s]\n"]}]},{"cell_type":"code","source":["# Make predictions\n","y_pred = xgb_model.predict(test_X)"],"metadata":{"id":"nvYxZcXYUNZi","executionInfo":{"status":"ok","timestamp":1688435546620,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","print(\"accuracy_score : \" + str(accuracy_score(test_y, y_pred)))"],"metadata":{"id":"Ii9xgWyvV6qH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688435546621,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}},"outputId":"a46c7111-c6bb-4752-e09d-c60f302c1d3a"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_score : 0.6075\n"]}]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","xgb = XGBClassifier(n_estimators = 100, learning_rate = 0.1, max_depth=3)\n","xgb.fit(train_X, train_y)\n","y_pred = xgb.predict(test_X)"],"metadata":{"id":"sTfqA0X5t2II","executionInfo":{"status":"ok","timestamp":1688435547088,"user_tz":-420,"elapsed":477,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["print(\"accuracy_score : \" + str(accuracy_score(test_y, y_pred)))"],"metadata":{"id":"XAO_AIC0uEAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688435547088,"user_tz":-420,"elapsed":2,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}},"outputId":"f28d5f12-3c84-47db-9d66-13c2b0ba5f32"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_score : 0.62\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rbn3T9I0uFeu","executionInfo":{"status":"ok","timestamp":1688435547089,"user_tz":-420,"elapsed":3,"user":{"displayName":"Quang Duy Tran","userId":"00584814694078639133"}}},"execution_count":52,"outputs":[]}]}